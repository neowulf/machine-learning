{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T22:54:12.322759Z",
     "start_time": "2017-12-15T22:53:53.654923Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Allows for interactive shell - outputs all non variable statements\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4, linewidth=100)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T22:54:12.336625Z",
     "start_time": "2017-12-15T22:54:12.324649Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "np.random.seed(10)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "DATASET_DIR=os.path.join(current_dir, 'dataset')\n",
    "CROSSVALID_DIR=os.path.join(DATASET_DIR, 'cross_valid')\n",
    "TRAIN_DIR = os.path.join(DATASET_DIR, 'train')\n",
    "TEST_DIR = os.path.join(DATASET_DIR, 'test')\n",
    "CROSSVALID_DIR = os.path.join(DATASET_DIR, 'cross_valid')\n",
    "SAMPLE_DIR = os.path.join(DATASET_DIR, 'sample')\n",
    "\n",
    "WEIGHTS_DIR = os.path.join(current_dir, 'weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the dataset\n",
    "\n",
    "```\n",
    "kg download -c 'dogs-vs-cats-redux-kernels-edition'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unzip training and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T02:19:31.277906Z",
     "start_time": "2017-12-15T02:19:21.527092Z"
    }
   },
   "outputs": [],
   "source": [
    "# Start from fresh\n",
    "!rm -rf dataset\n",
    "!mkdir dataset\n",
    "!unzip -q train.zip -d $DATASET_DIR\n",
    "!unzip -q test.zip -d $DATASET_DIR\n",
    "!tree -d\n",
    "\n",
    "!find dataset -maxdepth 5 -type d -exec sh -c \"echo '{}'; ls -1 '{}' | wc -l\" \\; | xargs -n 2 | awk '{print $1\" \"$2}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare the training, crossvalidation, sample dataset along with classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training, validation, sample batch dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. There are 12,500 images in the test set.\n",
    "1. There are 25,000 images in the train set.\n",
    "1. We need to create and move 10% of `train` to a cross validation set.\n",
    "1. We will also create a `sample` set containing 10% of the remaining `train` set which will be copied from `train`. The `sample` will be used to test the training process of the model before fully training the model using the `train` model.\n",
    "    1. `sample/train` can contain 200 samples from `train`.\n",
    "    1. `sample/cross_valid` can contain 50 samples from `train`.\n",
    "\n",
    "So, the directory structure would be:\n",
    "```\n",
    "dataset/train/\n",
    "dataset/cross_valid/\n",
    "dataset/sample/train/\n",
    "dataset/sample/cross_valid/\n",
    "\n",
    "dataset/test/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T02:19:45.119896Z",
     "start_time": "2017-12-15T02:19:44.718225Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_crossvalidation(perc = 0.1):\n",
    "    \"\"\"\n",
    "    moves `perc` of train dir to cross validation dir\n",
    "    \"\"\"\n",
    "    os.makedirs(CROSSVALID_DIR, exist_ok=True)\n",
    "    g = glob(os.path.join(TRAIN_DIR, '*.jpg'))\n",
    "    shuf = np.random.permutation(g)\n",
    "    for i in range(int(shuf.shape[0] * perc)):\n",
    "        filename = os.path.basename(shuf[i])\n",
    "        os.rename(shuf[i], os.path.join(CROSSVALID_DIR, filename))\n",
    "\n",
    "def create_sample(sample_train_size=200, sample_crossvalid_size=50):\n",
    "    \"\"\"\n",
    "    sample perc of train data is copied to sample directory\n",
    "    creates sample train and sample test directories\n",
    "    \"\"\"\n",
    "    \n",
    "    sample_train_dir = os.path.join(SAMPLE_DIR, 'train')\n",
    "    sample_crossvalid_dir = os.path.join(SAMPLE_DIR, 'cross_valid')\n",
    "    \n",
    "    g = glob(os.path.join(TRAIN_DIR, '*.jpg'))\n",
    "    shuf = np.random.permutation(g)\n",
    "    \n",
    "    ## SPLIT\n",
    "    train_set = shuf[0:sample_train_size]\n",
    "    crossvalid_set = shuf[sample_train_size:sample_train_size + sample_crossvalid_size]\n",
    "    \n",
    "    os.makedirs(sample_train_dir, exist_ok=True)\n",
    "    for i in train_set:\n",
    "        filename = os.path.basename(i)\n",
    "        shutil.copy(i, os.path.join(sample_train_dir, filename))\n",
    "    \n",
    "    os.makedirs(sample_crossvalid_dir, exist_ok=True)\n",
    "    for i in crossvalid_set:\n",
    "        filename = os.path.basename(i)\n",
    "        shutil.copy(i, os.path.join(sample_crossvalid_dir, filename))\n",
    "\n",
    "def create_labels(abs_directory, labels = ['cat', 'dog']):\n",
    "    \"\"\"\n",
    "    partitions the directories into new directory which is the label\n",
    "    \"\"\"\n",
    "    if labels is None:\n",
    "        label = 'unknown'\n",
    "        target_dir=os.path.join(abs_directory, label)\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        for file in glob(os.path.join(abs_directory, '*.jpg')):\n",
    "            target = os.path.join(target_dir, os.path.basename(file))\n",
    "            shutil.move(file, target)\n",
    "    else:\n",
    "        for label in labels:\n",
    "            target_dir=os.path.join(abs_directory, label)\n",
    "            os.makedirs(target_dir, exist_ok=True)\n",
    "            for file in glob(os.path.join(abs_directory, label + '.*.jpg')):\n",
    "                target = os.path.join(target_dir, os.path.basename(file))\n",
    "                shutil.move(file, target)\n",
    "    \n",
    "# Create the sample set\n",
    "create_sample()\n",
    "\n",
    "# Create the cross validation set\n",
    "create_crossvalidation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-15T02:52:07.173830Z",
     "start_time": "2017-12-15T02:52:07.115696Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create labeled directories for each of the sets\n",
    "create_labels(TRAIN_DIR)\n",
    "create_labels(CROSSVALID_DIR)\n",
    "create_labels(SAMPLE_DIR+'/train')\n",
    "create_labels(SAMPLE_DIR+'/cross_valid')\n",
    "create_labels(TEST_DIR, labels=None)\n",
    "\n",
    "!find $DATASET_DIR -maxdepth 5 -type d -exec \\\n",
    "   sh -c \"echo '{}'; ls -1 '{}' | wc -l\" \\; | xargs -n 2 | awk '{print $1\" \"$2}'"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "337px",
    "left": "997px",
    "right": "95px",
    "top": "97px",
    "width": "316px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
